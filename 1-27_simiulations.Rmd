---
title: "Adaptive Binning 1/21 Simulations"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE, error=FALSE)
setwd("/Users/marco/Dropbox/Duke/projects/FLAME-binning/")
source("AB_MIPs.R")
library(ggplot2)
library(reshape2)
library(dbarts)
library(RColorBrewer)
require(dbarts)
require(MatchIt)
require(beepr)
require(cem)
require(tidyverse)
source("matching_estimators.R")
#source("AB_MIP.R")
set.seed(42069)
```


```{r "sanity check", results='hide', eval=FALSE}
yi = 5
xi = c(2, 4)
p = 2
x_train = matrix(NA, 4, p)
x_train[, 1] = c(1, 2, 4, 5)
x_train[, 2] = c(2, 3, 1, 4)
z_train = c(1,1,1,1)
x_test = matrix(NA, 2, p)
x_test[, 1] = c(3, 1)
x_test[, 2] = c(4, 1)
y_train = c(1, 3, 2, 5)
n_train = length(y_train)
n_test = nrow(x_test)
z_test = c(0,0)

M=1e5
mip_pars = setup_miqp_variance(xi = xi, y_train = y_train, x_train = x_train, x_test =  x_test,
                              alpha =1, lambda=2, m=2, M=1e5)

rng = which(!names(mip_pars$bvec) %in% c())
sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat[rng, ], Qmat=mip_pars$Qmat,
             bvec=mip_pars$bvec[rng], sense = mip_pars$sense[rng],
             lb=mip_pars$lb, ub=mip_pars$ub,
           vtype = mip_pars$vtype, objsense = "max")
print(sol$status)
print(sol$xopt)

mip_out = recover_pars_variance(sol, nrow(x_train), nrow(x_test), p)
```


## 1-D Example MIQP-Variance

```{r "simple MIQP-V p=1", cache=TRUE, results='hide'}
n = 100
n_train = n
n_test = n
p = 1
x_train = matrix(rexp(n*p, 2), n, p)
y_train = rowSums(log(x_train)) + rnorm(n, 0, 1)
x_test = matrix(rexp(n*p, 2), n, p)
y_test = rowSums(log(x_test)) + rnorm(n, 0, 1)
z_train = rep(1, n)
z_test = rep(0, n_test)
simdata = matrix(NA, n, 3* p + 2)
simdata = data.frame(simdata)
names(simdata) = c(paste("x",1:p, sep=""), 
                   paste("a", 1:p, sep=""), 
                   paste("b", 1:p, sep=""), "y", "yest")
for (i in 1:n){
  print(i)
  xi = x_test[i,]
  
  t1 = proc.time()[3]
  mip_pars = setup_miqp_variance(xi = xi, y_train = y_train, x_train = x_train, 
                                x_test =  x_test, lambda=2, alpha=0, m=1, M=1e5)
  t2 = proc.time()[3]
  sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat, Qmat = mip_pars$Qmat,
               bvec=mip_pars$bvec, sense = mip_pars$sense,
               lb=mip_pars$lb, ub=mip_pars$ub,
               vtype = mip_pars$vtype, objsense = "max")
  t3 = proc.time()[3]
  
  mip_out = recover_pars_variance(sol, n_train, n_test, p)

  yest = mean(y_test[which(mip_out$w>=0.1)])
  simdata[i, ] = c(xi, mip_out$a, mip_out$b, y_test[i], yest)
  
  t4 = proc.time()[3]
  print(paste("Par generation took", round(t2 - t1, 1), 
              "seconds, solving mip took", round(t3 - t2, 1), "seconds",
              "total:", round(t4 - t1), "seconds"))
  
}
```

```{r "simple MIQP-V p=1 plot"}
ggplot(simdata) + 
  geom_rect(aes(xmin=a1, ymin=min(y), xmax=b1, ymax=max(y)), color="black", size=0.5, alpha=0.3, fill="grey") +
  geom_point(aes(x=x1, y=y), color='red', size=2) + 
  geom_point(aes(x=x1, y=yest), color="blue") + 
  geom_point(data=data.frame(x_train, y_train),aes(x=x_train, y=y_train), pch=18) +
  xlab("x") + ylab("y") + theme_bw() + 
  theme(legend.position = c(0.8,0.2), legend.background = element_rect(color="black", size=0.5),
        legend.title = element_blank())

```


## 1-D Example MIP-Explain

```{r "simple MIP-E p=1", cache=TRUE, results='hide'}
n = 100
n_train = n
n_test = n
p = 1
x_train = matrix(rexp(n*p, 2), n, p)
y_train = rowSums(log(x_train)) + rnorm(n, 0, 1)
x_test = matrix(rexp(n*p, 2), n, p)
y_test = rowSums(log(x_test)) + rnorm(n, 0, 1)
z_train = rep(1, n)
z_test = rep(0, n_test)
simdata = matrix(NA, n, 3* p + 2)
simdata = data.frame(simdata)
names(simdata) = c(paste("x",1:p, sep=""), 
                   paste("a", 1:p, sep=""), 
                   paste("b", 1:p, sep=""), "y", "yest")

bt = bart(data.frame(x_train), y_train, keeptrees = T, verbose = FALSE)
fhat = predict(bt, x_test)
for (i in 1:n){
  print(i)
  xi = x_test[i,]
    
  t1 = proc.time()[3]
  mip_pars = setup_mip_explain(fhat=fhat[i], xi = xi, y_test=y_test[-i], 
                               x_test = x_test[-i, ,drop=F], lambda=2,  m=1, M=1e5)
  t2 = proc.time()[3]
  
  sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat,
               bvec=mip_pars$bvec, sense = mip_pars$sense,
               lb=mip_pars$lb, ub=mip_pars$ub,
               vtype = mip_pars$vtype, objsense = "max")
  t3 = proc.time()[3]
  
  mip_out = recover_pars_variance(sol, n_train, n_test-1, p)

  yest = mean(y_test[-i][which(mip_out$w>=0.1)])
  simdata[i, ] = c(xi, mip_out$a, mip_out$b, y_test[i], yest)
  
  t4 = proc.time()[3]
  print(paste("Par generation took", round(t2 - t1, 1), 
              "seconds, solving mip took", round(t3 - t2, 1), "seconds",
              "total:", round(t4 - t1), "seconds"))
  
}
```

```{r "simple MIP-E p=1 plot"}
ggplot(simdata) + 
  geom_rect(aes(xmin=a1, ymin=min(y), xmax=b1, ymax=max(y)), color="black", size=0.5, alpha=0.3, fill="grey") +
  geom_point(aes(x=x1, y=y), color='red', size=2) + 
  geom_point(aes(x=x1, y=yest), color="blue") + 
  geom_point(data=data.frame(x_train, y_train),aes(x=x_train, y=y_train), pch=18) +
  xlab("x") + ylab("y") + theme_bw() + 
  theme(legend.position = c(0.8,0.2), legend.background = element_rect(color="black", size=0.5),
        legend.title = element_blank())

```


## 1-D Example MIP-Predict

```{r "simple MIP-P p=1", cache=TRUE, results='hide'}
n = 100
n_train = n
n_test = n
p = 1
x_train = matrix(rexp(n*p, 2), n, p)
y_train = rowSums(log(x_train)) + rnorm(n, 0, 1)
x_test = matrix(rexp(n*p, 2), n, p)
y_test = rowSums(log(x_test)) + rnorm(n, 0, 1)
z_train = rep(1, n)
z_train[sample(1:n_train, n_train/2, replace=T)] = 0
z_test = rep(0, n_test)
simdata = matrix(NA, n, 3* p + 2)
simdata = data.frame(simdata)
names(simdata) = c(paste("x",1:p, sep=""), 
                   paste("a", 1:p, sep=""), 
                   paste("b", 1:p, sep=""), "y", "yest")

for (i in 1:n){
  print(i)
  xi = x_test[i,]
    
  t1 = proc.time()[3]
  mip_pars = setup_mip_predict(xi = xi, yi = y_test[i], zi=1, 
                               x_train = x_train, y_train=y_train, z_train=z_train, 
                               x_test = x_test[-i, ,drop=F], 
                               lambda1=10, lambda2=10, alpha=0,  m=1, M=1e5)
  t2 = proc.time()[3]
  
  sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat,
               bvec=mip_pars$bvec, sense = mip_pars$sense,
               lb=mip_pars$lb, ub=mip_pars$ub,
               vtype = mip_pars$vtype, objsense = "max")
  t3 = proc.time()[3]
  
  mip_out = recover_pars_variance(sol, n_train, n_test-1, p)

  yest = mean(y_test[-i][which(mip_out$w>=0.1)])
  simdata[i, ] = c(xi, mip_out$a, mip_out$b, y_test[i], yest)
  
  t4 = proc.time()[3]
  print(paste("Par generation took", round(t2 - t1, 1), 
              "seconds, solving mip took", round(t3 - t2, 1), "seconds",
              "total:", round(t4 - t1), "seconds"))
  
}
```

```{r "simple MIP-P p=1 plot"}
ggplot(simdata) + 
  geom_rect(aes(xmin=a1, ymin=min(y), xmax=b1, ymax=max(y)), color="black", size=0.5, alpha=0.3, fill="grey") +
  geom_point(aes(x=x1, y=y), color='red', size=2) + 
  geom_point(aes(x=x1, y=yest), color="blue") + 
  geom_point(data=data.frame(x_train, y_train),aes(x=x_train, y=y_train), pch=18) +
  xlab("x") + ylab("y") + theme_bw() + 
  theme(legend.position = c(0.8,0.2), legend.background = element_rect(color="black", size=0.5),
        legend.title = element_blank())

```


## 2D Example MIP-Explain

```{r "simple MIP-E p=2", cache=TRUE, results='hide'}
n = 100
n_train = 100
n_test = 100
p = 2
x_train = matrix(rexp(n_train*p, 2), n_train, p)
y_train = rowSums(log(x_train)) + rnorm(n_train, 0, 1)
x_test = matrix(rexp(n_test*p, 2), n_test, p)
y_test = rowSums(log(x_test)) + rnorm(n_test, 0, 1)
z_train = rep(1, n_train)
z_test = rep(0, n_test)
simdata = matrix(NA, n_test, 3* p + 2)
simdata = data.frame(simdata)
names(simdata) = c(paste("x",1:p, sep=""), 
                   paste("a", 1:p, sep=""), 
                   paste("b", 1:p, sep=""), "y", "yest")
bt = bart(data.frame(x_train), y_train, keeptrees = T, verbose = FALSE)
fhat = predict(bt, x_test)
t1 = proc.time()[3]
for (i in 1:n_test){
  message(paste("Matching unit", i, "of", n_test), "\r", appendLF = FALSE); flush.console()
  xi = x_test[i,]

  mip_pars = setup_mip_explain(fhat=fhat[i], xi = xi,  x_test =  x_test[-i, , drop=F], y_test=y_test[-i],
                                lambda=5, m=1, M=1e5)
  sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat,
               bvec=mip_pars$bvec, sense = mip_pars$sense,
               lb=mip_pars$lb, ub=mip_pars$ub,
               vtype = mip_pars$vtype, objsense = "max", control=list(trace=0))
  mip_out = recover_pars_variance(sol, n_train, n_test-1, p)
  
  yest = mean(y_test[-i][which(mip_out$w>=0.1)])
  simdata[i, ] = c(xi, mip_out$a, mip_out$b, y_test[i], yest)
}
print(paste("100 matches took", round(proc.time()[3] - t1, 1)))
```

```{r "simple MIP-E p=2 plot"}
ggplot(simdata) + 
  geom_rect(aes(xmin=a1, ymin=a2, xmax=b1, ymax=b2), fill="grey", 
            color="black", size=0.5, alpha=0.3) + 
  geom_point(aes(x=x1, y=x2, color=abs(yest - y)/mean(abs(y))), size=2) + 
  scale_color_gradient(low="blue", high="red") + 
  xlab("x1") + ylab("x2") +  labs(color="% Abs. error") +  theme_bw() + 
  theme(legend.position = c(0.9,0.5), 
        legend.background = element_rect(color="black", size=0.5))
#ggsave("01-21_p=2_plot.png")
```

## 2D Example MIP-Predict

```{r "simple MIP-P p=2", cache=TRUE, results='hide'}
n = 100
n_train = 100
n_test = 100
p = 2
x_train = matrix(rexp(n_train*p, 2), n_train, p)
y_train = rowSums(log(x_train)) + rnorm(n_train, 0, 1)
x_test = matrix(rexp(n_test*p, 2), n_test, p)
y_test = rowSums(log(x_test)) + rnorm(n_test, 0, 1)
z_train = rep(1, n_train)
z_train[sample(1:n_train, n_train/2, replace = FALSE)] = 0
z_test = rep(0, n_test)
simdata = matrix(NA, n_test, 3* p + 2)
simdata = data.frame(simdata)
names(simdata) = c(paste("x",1:p, sep=""), 
                   paste("a", 1:p, sep=""), 
                   paste("b", 1:p, sep=""), "y", "yest")
t1 = proc.time()[3]
for (i in 1:n_test){
  message(paste("Matching unit", i, "of", n_test), "\r", appendLF = FALSE); flush.console()
  xi = x_test[i,]

  mip_pars = setup_mip_predict(xi = xi, zi=1, yi=y_test[i],
                               x_train=x_train, y_train = y_train, z_train=z_train,
                               x_test =  x_test[-i, , drop=F],
                               lambda1=10, lambda2=10, m=1, M=1e5)
  sol = Rcplex(cvec=mip_pars$cvec, Amat=mip_pars$Amat,
               bvec=mip_pars$bvec, sense = mip_pars$sense,
               lb=mip_pars$lb, ub=mip_pars$ub,
               vtype = mip_pars$vtype, objsense = "max", control=list(trace=0))
  mip_out = recover_pars_variance(sol, n_train, n_test-1, p)
  
  yest = mean(y_test[-i][which(mip_out$w>=0.1)])
  simdata[i, ] = c(xi, mip_out$a, mip_out$b, y_test[i], yest)
}
print(paste("100 matches took", round(proc.time()[3] - t1, 1)))
```

```{r "simple MIP-P p=2 plot"}
ggplot(simdata) + 
  geom_rect(aes(xmin=a1, ymin=a2, xmax=b1, ymax=b2), fill="grey", 
            color="black", size=0.5, alpha=0.3) + 
  geom_point(aes(x=x1, y=x2, color=abs(yest - y)/mean(abs(y))), size=2) + 
  scale_color_gradient(low="blue", high="red") + 
  xlab("x1") + ylab("x2") +  labs(color="% Abs. error") +  theme_bw() + 
  theme(legend.position = c(0.9,0.5), 
        legend.background = element_rect(color="black", size=0.5))
#ggsave("01-21_p=2_plot.png")
```


```{r}
# Functions ---------------------------------------------------------------
expit <- function(a, x) {
  exp(a * x) / (1 + exp(a * x))
}

expansion_variance <- function(current_bin, expanded_bin, df, bart_fit) {
  total_var <- 0
  p = ncol(df)-2
  for (j in 1:p) {
    if (all(current_bin[j, ] == expanded_bin[j, ])) {
      # print('tada')
      next
    }
    expanded <- which(current_bin[j, ] != expanded_bin[j, ])
    grid_pts <- seq(current_bin[j, expanded], expanded_bin[j, expanded], 
                    length.out = 8)
    bin_centers <- rowMeans(current_bin)
    
    pred_data <- 
      sapply(grid_pts, function(x) {
        bin_centers[j] <- x
        bin_centers
      }) %>%
      t() %>% 
      as.data.frame() %>%
      mutate(treatment = 1) %>%
      `colnames<-`(colnames(dplyr::select(df, -Y)))
    
    total_var <- total_var + var(colMeans(predict(bart_fit, pred_data)))
  }
  return(total_var)
}
# Algorithm ---------------------------------------------------------------


matching_sim <- function(n_sims = 10, n_units = 100, p = 3, n_train = floor(n_units / 2)) {
  
  n_test <- n_units - n_train
  n_estimators <- 9 # CEM, Dynamic Binning, Full Matching, 
                    # Mahalanobis, Nearest Neighbor, Prognostic, 
                    # MIP-Predict, # MIP-Explain, # MIP-Variance

  alpha <- 2 # Baseline response
  beta_tilde <- 5 # treatment effect
  beta <- runif(p, -1, 1) # To go from X to propensity score 
  
  # Effect of covariates on outcome; not always used 
  gamma <- matrix(runif(p, -3, 3), nrow = p)
  
  for (sim in 1:n_sims) {
    ## For generating propensity scores and assigning treatment
    X <- matrix(runif(p * n_units, 0, 5), nrow = n_units)
    e <- expit(.01, X %*% beta)
    Z <- rbinom(n_units, 1, e)
    
    ## Generate outcome 
    HTE <- (X[, 1] > 1.5) * beta_tilde * Z
    HTE_true <- HTE[intersect((n_train + 1):n_units, which(Z == 1))]
    Y <- alpha + HTE + rnorm(n_units, 0, 1)
    
    df <- cbind(data.frame(X), data.frame(Y = Y, treated = as.logical(Z)))
    
    # Formula for matching methods
    f <- formula(paste('treated ~', paste(colnames(df)[1:p], collapse = ' + '))) 
    
    train_df <-
      df %>%
      slice(1:n_train)
    
    train_covs <- 
      train_df %>%
      dplyr::select(1:p)
    
    train_control <- which(!train_df$treated)
    train_treated <- which(train_df$treated)
    
    test_df <-
      df %>%
      slice((n_train + 1):n_units)
    
    test_covs <- 
      test_df %>%
      dplyr::select(1:p)
    
    test_control <- which(!test_df$treated)
    test_treated <- which(test_df$treated)
    
    n_test_control <- length(test_control)
    n_test_treated <- length(test_treated)
    
    store_all_HTEs <- NULL
    
    bart_fit <- bart(x.train = dplyr::select(train_df, -Y),
                     y.train = train_df$Y,
                     x.test = mutate(test_df[test_df$treated, 1:p], treated = 0), # Prognostic score on test units 
                     keeptrees = TRUE,
                     verbose = FALSE)
    
    message("Running alternative estimators...")
    ################### Alternative Matching Estimators ################### 
    HTE_fullmatch <- est_fullmatch(test_df, f)
    HTE_prog <- est_prog(test_df, bart_fit$yhat.test.mean)
    HTE_cem <- est_cem(test_df)
    HTE_mahal <- est_mahal(f, test_df)
    HTE_nn <- est_nn(f, test_df, ratio = 1)
    # HTE_caliper <- est_caliper(f, df, ratio = 1) # Doesn't work...? 
    ######################################################################
    
    
    # Initialize bins to be centered right on treated unit values 
    bins <- array(dim = c(n_test_treated, p, 2))
    bins[, , 1] <- as.matrix(test_df[test_treated, 1:p])
    bins[, , 2] <- as.matrix(test_df[test_treated, 1:p])
    
    # Indices of test units already matched to, for each test, treated unit
    # Every unit will trivially be in their own MG 
    already_matched <- as.list(test_treated)
    
    # Idea: Don't consider test control units whose bins are strictly larger than 
    #   those of test control units you've already co6nsidered
    #   This should slow things down, however, so table it for now. 
    message("Running Greedy AB...")
    for (i in 1:n_test_treated) {
      message(paste("Matching unit", i, "of", n_test_treated), "\r", appendLF = FALSE); flush.console()
      bin_copy <- matrix(bins[i, ,], ncol = 2) # For case of 1 covariate
      counter <- 0
      while (length(already_matched[[i]]) < 5) { # Variance is not too big
        min_var <- Inf
        min_size_increase <- Inf
        
        # Find units closest along each axis
        potential_matches <- setdiff(test_control, already_matched[[i]])
        bin_var <- vector('numeric', length = p)
        proposed_bin <- vector('list', length = p)
        for (j in 1:p) {
          expand_up <- intersect(potential_matches, which(test_df[, j] > bin_copy[j, 2]))
          if (length(expand_up) == 0) {
            distance_up <- Inf
          }
          else {
            distance_up <- min(abs(bin_copy[j, 2] - test_df[expand_up, j])) ###### Consider sorting covariates so we don't have to do this   
          }
          
          expand_down <- intersect(potential_matches, which(test_df[, j] < bin_copy[j, 1]))
          
          if (length(expand_down) == 0) { # Can't go any lower
            distance_down <- Inf
          }
          else {
            distance_down <- min(abs(bin_copy[j, 1] - test_df[expand_down, j]))  
          }
          # So what happens is that one direction is really responsible for the treatment effect 
          # and BART knows this so you never expand on it and then you end up making the other bins trivially large 
          if (is.infinite(distance_up) & is.infinite(distance_down)) { 
            bin_var[j] <- Inf
            next
          }
          
          if (distance_up < distance_down) {
            proposed_bin[[j]] <- bin_copy
            proposed_bin[[j]][j, 2] <- test_df[expand_up[which.min(abs(bin_copy[j, 2] - test_df[expand_up, j]))], j]
          }
          else {
            proposed_bin[[j]] <- bin_copy
            proposed_bin[[j]][j, 1] <- test_df[expand_down[which.min(abs(bin_copy[j, 1] - test_df[expand_down, j]))], j]
          }
          # browser()
          bin_var[j] <- expansion_variance(bin_copy, proposed_bin[[j]], train_df, bart_fit)
        }
        
        expand_along <- which.min(bin_var)
        # browser()
        bin_copy <- proposed_bin[[expand_along]]
        in_MG <- apply(test_covs, 1, function(x) all(x >= bin_copy[, 1]) & all(x <= bin_copy[, 2]))
        # browser()
        already_matched[[i]] <- unique(c(already_matched[[i]], which(in_MG)))
        
        counter <- counter + 1
      }
      
      bins[i, , ] <- bin_copy
    }
    message("\n")
    
    CATE <- vector('numeric', n_test_treated)
    size <- vector('numeric', n_test_treated)
    for (i in 1:n_test_treated) {
      in_MG <- which(apply(test_covs, 1, function(x) all(x >= bins[i, , 1]) & all(x <= bins[i, , 2])))
      size[i] <- length(in_MG)
      treated <- in_MG[which(test_df$treated[in_MG])]
      control <- in_MG[which(!test_df$treated[in_MG])]
      CATE[i] <- mean(test_df$Y[treated]) - mean(test_df$Y[control])
    }
    ATE <- sum(CATE * size) / sum(size)
    
    ## MIP-Predict
    HTE_mip_p = vector('numeric', n_test_treated)
    message("Running MIP-Predict...")
    for (l in 1:n_test_treated){
      i = test_treated[l]
      message(paste("Matching unit", l, "of", n_test_treated), "\r", appendLF = FALSE); flush.console()
      
      mip_pars =  setup_mip_predict(xi = as.numeric(test_covs[i, ]), 
                                    zi =  1,  y_train = train_df$Y, 
                                    x_train = as.matrix(train_covs), z_train = train_df$treated, 
                                    x_test = as.matrix(test_covs[test_df$treated==0]),  
                                    alpha=0, lambda1=10, lambda2=10, m=1, M=1e5)
      sol <- do.call(Rcplex, c(mip_pars, list(objsense="max", control=list(trace=0))))
      mip_out = recover_pars(sol, n_train, n_test, p)
    
      HTE_mip_p[l] = test_df$Y[i] - mean(test_df$Y[test_df$treated==0][mip_out$w>=0.1])
    }
    message("\n")
    
    ## MIP-Explain
    HTE_mip_p = vector('numeric', n_test_treated)
    message("Running MIP-Predict...")
    bt = bart(data.frame(train_covs, treated=train_df$treated), 
              train_df$Y, keeptrees = T, verbose = FALSE)
    fhat = predict(bt, data.frame(test_covs, treated=test_df$treated))
    for (l in 1:n_test_treated){
      i = test_treated[l]
      message(paste("Matching unit", l, "of", n_test_treated), "\r", appendLF = FALSE); flush.console()
      
      mip_pars =  setup_mip_explain(fhat = fhat[i], xi = as.numeric(test_covs[i, ]), 
                                    y_test = test_df$Y[test_df$treated==0],
                                    x_test = as.matrix(test_covs[test_df$treated==0]),  
                                    alpha=0, lambda=10, m=1, M=1e5)
      sol <- do.call(Rcplex, c(mip_pars, list(objsense="max", control=list(trace=0))))
      mip_out = recover_pars(sol, n_train, n_test, p)
    
      HTE_mip_p[l] = test_df$Y[i] - mean(test_df$Y[test_df$treated==0][mip_out$w>=0.1])
    }
    message("\n")
    
    this_sim <- 
      rbind(cbind(HTE_true, CATE), # dynamic binning
            cbind(HTE_true, MIP_cates),
            cbind(HTE_true, HTE_fullmatch), 
            cbind(HTE_true, HTE_prog),
            cbind(HTE_true, HTE_cem), 
            cbind(HTE_true, HTE_mahal), 
            cbind(HTE_true, HTE_nn)) %>%
      as.data.frame() %>%
      `colnames<-`(c('actual', 'predicted')) %>%
      mutate(estimator = rep(c('Dynamic Binning','MIP DB',
                               'Full Matching', 'Prognostic',
                               'CEM', 'Mahalanobis', 'Nearest Neighbor'),
                             each = nrow(.) / n_estimators))
    store_all_HTEs = rbind(store_all_HTEs, this_sim)
    
    print(sprintf('%d of %d simulations completed', sim, n_sims))
  }
  beep()
  
  store_all_HTEs %>%
    group_by(estimator) %>%
    summarize(MSE = mean((actual - predicted) ^ 2, na.rm = TRUE),
              percent_missing = 100 * mean(is.na(predicted))) %>%
    arrange(MSE) %>%
    return()
}

# Analysis ----------------------------------------------------------------

#res = matching_sim(n_sims = 10, n_units = 100, p = 5)

# unique_HTEs <- unique(HTE$actual)
# if (length(unique_HTEs) < nrow(HTE) / n_estimators) { # Constant treatment effect 
#   gg <- 
#     ggplot(HTE, aes(x = as.factor(actual), y = predicted, color = estimator)) + 
#     geom_boxplot()
#   for (i in 1:length(unique_HTEs)) {
#     gg <- gg + 
#       geom_hline(yintercept = unique_HTEs[i])
#   }
#   gg <- gg + 
#     labs(x = 'Actual', 
#          y = 'Predicted',
#          title = '(Piecewise-)Constant Treatment Effect')
#   print(gg)
# } else {
#   ggplot(HTE, aes(x = actual, y = predicted)) + 
#     geom_point(aes(color = estimator)) + 
#     geom_abline(intercept = 0, slope = 1, color = 'black') + 
#     labs(title = 'Heterogeneous Treatment Effect')
# }
# 
# partitions <- 
#   bins %>%
#   c() %>%
#   unique()
# 
# g <- ggplot(data = df, aes(x = X, y = Y)) + 
#   geom_point(aes(color = treated))
# for (partition in partitions) {
#   g <- g + geom_vline(xintercept = partition)
# }
# plot(g)
```

